---
#### 1. Utilisation de l'API `_analyze`
---

<details>
<summary>RAPPELS</summary>


* L'analyse dans Elasticsearch est seulement applicable aux champs textuels.
* Lorsqu'un document est indexÃ© les valeurs textuels sont analysÃ©es.
* Le rÃ©sulat est stockÃ© dans des structures de donnÃ©es pour rendre la recherche efficiente.

<img src="https://i.ibb.co/zmQtMSk/01-Screenshot-from-2021-03-18-11-00-54.png" width="60%">

RÃ©fÃ©rence :  
Documentation des analyzers embarquÃ©s dans Elasticsearch :<br/>
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html

###### :arrow_forward: DÃ©coupage en tokens d'un texte avec le tokenizer de type `standard`

```
POST _analyze
{
  "tokenizer": "standard",
  "text": "I'm in the mood for drinking semi-dry red wine!"
}
```
<img src="https://i.ibb.co/9Y79z3c/041-Screenshot-2021-03-17-Elastic-Kibana.png" width="40%">

A noter que le tokenizer stocke Ã©galement l'offset de chaque token.

###### :arrow_forward: Utilisation du filtre `lowercase`

Un filtre reÃ§oit les donnÃ©es du tokenizer, il peut les filtrer ou les modifier.<br/>
Un analyzer peut contenir aucun ou plusieurs filtres.

```
POST _analyze
{
  "filter": [ "lowercase" ],
  "text": "I'm in the mood for drinking semi-dry red wine!"
}
```

<img src="https://i.ibb.co/TRF52QD/042-Screenshot-2021-03-17-Elastic-Kibana.png" width="60%">

###### :arrow_forward: Utilisation d'un analyzer de type `standard`

```
POST _analyze
{
  "analyzer": "standard",
  "text": "I'm in the mood for drinking semi-dry red wine!"
}
```

<img src="https://i.ibb.co/Q96GqQW/043-Screenshot-2021-03-17-Elastic-Kibana.png" width="30%">

Pour rÃ©sumer l'action du standard analyzer :

<img src="https://i.ibb.co/6ZTgZdz/02-Screenshot-from-2021-03-18-11-13-22.png" width="80%">

</details>

---
#### 2. Jouons avec les Analyzers
---
<details>
<summary>TP</summary>

- :arrow_right: Lors des exercices suivants n'hÃ©sitez pas Ã  expÃ©rimenter et utiliser vos propres phrases  
  ainsi que l'analyseur de langue franÃ§aise
  
###### :arrow_forward: Configurer un analyzer de type `standard`

```
PUT /analyzers_test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "english_stop": {
          "type": "standard",
          "stopwords": "_english_"
        }
      },
      "filter": {
        "my_stemmer": {
          "type": "stemmer",
          "name": "english"
        }
      }
    }
  }
}
```

**Rappels :  

** Les stopwords sont les mots qui vont Ãªtre filtrÃ©s durant l'analyse de texte.<br/> 
Exemple, en anglais ce sera : "the", "on", "of", "a", etc.

** Le stemming consiste a revenir Ã  la racine des mots (conjuguÃ©, accordÃ©s, dÃ©rivÃ©s, etc.)

###### :arrow_forward: Tester l'analyzer

```
POST /analyzers_test/_analyze
{
  "analyzer": "english_stop",
  "text": "I'm in the mood for drinking semi-dry red wine!"
}
```

<img src="https://i.ibb.co/vmGgWk4/044-Screenshot-2021-03-17-Elastic-Kibana.png" width="30%">

```
POST /analyzers_test/_analyze
{
  "tokenizer": "standard",
  "filter": [ "my_stemmer" ],
  "text": "I'm in the mood for drinking semi-dry red wine!"
}
```
Mettre un 's' Ã  la fin de wine pour vÃ©rifier que le stemming fonctionne bien.<br>
**Rappel :** Le stemming a pour fonction de rÃ©duire les mots Ã  leur racine. Exemple : "loved", "loves", "loving" vont converger vers "love".

<img src="https://i.ibb.co/BqRPRKf/045-Screenshot-2021-03-17-Elastic-Kibana.png" width="30%">

</details>

---
#### 3. Creation d'un analyzer custom
---
<details>
<summary>TP</summary>

###### :arrow_forward: Ajouter un analyzer configurÃ©

```
PUT /analyzers_test
{
  "settings": {
    "analysis": {
      "filter": {
        "my_stemmer": {
          "type": "stemmer",
          "name": "english"
        }
      },
      "analyzer": {
        "english_stop": {
          "type": "standard",
          "stopwords": "_english_"
        },
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "char_filter": [
            "html_strip"
          ],
          "filter": [
            "lowercase",
            "trim",
            "my_stemmer"
          ]
        }
      }
    }
  }
}
```

Utiliser le filtre HTML strip :<br/>
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-htmlstrip-charfilter.html

```
A complÃ©ter...
```

###### :arrow_forward: Tester l'analyzer configurÃ©

```
POST /analyzers_test/_analyze
{
  "analyzer": "my_analyzer",
  "text": "I'm in the mood for drinking <strong>semi-dry</strong> red wine!"
}
```

<img src="https://i.ibb.co/p2gmhLh/03-Screenshot-2021-03-18-Elastic-Kibana.png" width="30%">

</details>

---
#### 4. Utiliser des analyzers dans les mappings
---
<details>
<summary>TP</summary>

###### :arrow_forward: Utiliser un analyzer configurÃ© dans le mapping d'un champ

```
PUT /analyzers_test/_doc/_mapping
{
  "properties": {
    "description": {
      "type": "text",
      "analyzer": "my_analyzer"
    },
    "teaser": {
      "type": "text",
      "analyzer": "standard"
    }
  }
}
```

###### :arrow_forward: Ajouter un document de test

```
POST /analyzers_test/_doc/1
{
  "description": "drinking",
  "teaser": "drinking"
}
```

##### :arrow_forward: Tester le mapping

```
GET /analyzers_test/_search
{
  "query": {
    "term": {
      "teaser": {
        "value": "drinking"
      }
    }
  }
}
```

<img src="https://i.ibb.co/hZtYK2L/048-Screenshot-2021-03-17-Elastic-Kibana.png" width="30%">

```
GET /analyzers_test/_doc/_search
{
  "query": {
    "term": {
      "description": {
        "value": "drinking"
      }
    }
  }
}
```

<img src="https://i.ibb.co/YL9hy4L/049-Screenshot-2021-03-17-Elastic-Kibana.png" width="30%">

Essayez avec "drink" et Ã§a ira mieux. ;)


</details>

---
#### 5. TP : Construire un bon analyzer franÃ§ais pour Elasticsearch
---
<details>
<summary>TP</summary>

(By Joli Code)


Dans un index de recherche tel quâ€™Elasticsearch, une recherche full-text est une simple collecte de documents, qui sâ€™effectue via une comparaison de tokens.

Ces tokens vivent dans lâ€™index inversÃ© et ont Ã©tÃ© extraits du contenu de vos documents lors de lâ€™indexation. Plus vos tokens sont proprement indexÃ©s, et plus facilement un utilisateur trouvera vos documents : câ€™est le rÃ´le de lâ€™analyse.

Ce TP va vous guider dans la conception dâ€™un analyzer Elasticsearch pour la langue franÃ§aise qui soit Ã  la fois tolÃ©rant, pertinent et rapide â€“ et bien meilleur que lâ€™analyzer Â« french Â» fourni par dÃ©faut dans le moteur de recherche.


Lâ€™importance de lâ€™analyse
Prenons un document type pour commencer : le burger ğŸ”.
```
{
  "name": "Hamburger",
  "description": "Un hamburger, parfois hambourgeois (au Canada francophone) 
ou par aphÃ©rÃ¨se burger, est un sandwich d'origine allemande, composÃ© de deux 
pains de forme ronde (bun) parfois garnis de viande hachÃ©e (souvent du bÅ“uf) 
et gÃ©nÃ©ralement de cruditÃ©s â€” salade, tomate, oignon, cornichon (pickles) â€”, 
de fromage et de sauce. C'est un plat typique de la restauration rapide, 
emblÃ©matique de la cuisine amÃ©ricaine."
}
```
Avec lâ€™analyse par dÃ©faut (appelÃ©e Â« standard Â»), notre index va Ãªtre constituÃ© des mots simplement mis en minuscule. Pour nâ€™en citer que quelques-uns, par exemple :

sandwichâ€¯;
composÃ©â€¯;
cruditÃ©sâ€¯;
amÃ©ricaineâ€¯;
bÅ“uf.
Lors dâ€™une recherche, les termes recherchÃ©s sont analysÃ©s aussi, avec la mÃªme technique. Rechercher Â« Sandwichs Â» au pluriel donnerait le token sandwichs, qui nâ€™existe pas dans notre index. Lâ€™utilisateur va donc devoir saisir les mots exacts : avec pluriels, accents, ligatureâ€¦ Cela nâ€™est bien sÃ»r pas acceptableâ€¯!

En utilisant lâ€™analyzer french dâ€™Elasticsearch, les tokens seront plutÃ´t :

sandwichâ€¯;
composâ€¯;
cruditâ€¯;
americainâ€¯;
bÅ“uf.

Il y a une nette amÃ©lioration pour trois tokens : composÃ© est devenu compos, son lexÃ¨me (ou racine linguistique). Cela va nous permettre de trouver un burger en cherchant nâ€™importe quelle forme de ce mot : Â« composer Â», Â« compose Â»â€¦ 
<br>Mais quelques problÃ¨mes subsistent. Par exemple lâ€™e dans lâ€™o de bÅ“uf nâ€™est pas dÃ©composÃ©, et il sera donc impossible de trouver notre document en recherchant Â« boeuf Â»â€¯!

Câ€™est grÃ¢ce Ã  lâ€™analyse que les pluriels, les conjugaisons, la casseâ€¦ peuvent Ãªtre gÃ©rÃ©s. Voyons comment la construire et lâ€™amÃ©liorer.

### Les diffÃ©rentes Ã©tapes de lâ€™analyse

Lâ€™analyse menÃ©e par Elasticsearch se dÃ©compose en trois Ã©tapes successives :

1. Les Char Filter
Un char_filter permet dâ€™appliquer des transformations sur le texte complet, avant quâ€™il ne soit dÃ©coupÃ© en tokens. Cette Ã©tape permet de nettoyer le contenu, remplacer certains raccourcis, enlever du HTML ou de la ponctuation mal venue, etc.

Il serait par exemple possible de remplacer Â« & Â» par Â« et Â», afin dâ€™indexer lâ€™esperluette.

2. Le Tokenizer
Lâ€™Ã©tape du tokenizer consiste Ã  couper le texte en tokens. Elasticsearch utilise par dÃ©faut le standard Unicode Text Segmentation, qui va retirer la ponctuation et couper Ã  chaque espace.

La grande majoritÃ© des espaces est gÃ©rÃ©e, mais certains caractÃ¨res, comme lâ€™invisible trait dâ€™union conditionnel (Soft hyphen) seront conservÃ©sâ€¯! Et cela va poser de sÃ©rieux problÃ¨mes pour les Ã©tapes suivantes. Il en est de mÃªme pour le point mÃ©dianâ€¯!

3. Les Token Filter
Câ€™est lÃ  que la majoritÃ© du travail de nettoyage et dâ€™enrichissement sâ€™effectue lors de lâ€™analyse. Les token_filter peuvent modifier, ajouter et supprimer des tokens â€“ leur rÃ´le est donc multiple et leur ordre dâ€™exÃ©cution important : il sâ€™agit dâ€™une chaÃ®ne de filtres.

### Lâ€™analyzer Â« french Â» revisitÃ©
Lâ€™analyzer prÃ©-configurÃ© dans Elasticsearch (version 5.1 Ã  lâ€™heure oÃ¹ jâ€™Ã©cris ces lignes) est le suivant :
```
{
  "settings": {
    "analysis": {
      "filter": {
        "french_elision": {
          "type":         "elision",
          "articles_case": true,
          "articles": [
              "l", "m", "t", "qu", "n", "s",
              "j", "d", "c", "jusqu", "quoiqu",
              "lorsqu", "puisqu"
            ]
        },
        "french_stop": {
          "type":       "stop",
          "stopwords":  "_french_" 
        },
        "french_keywords": {
          "type":       "keyword_marker",
          "keywords":   [] 
        },
        "french_stemmer": {
          "type":       "stemmer",
          "language":   "light_french"
        }
      },
      "analyzer": {
        "french": {
          "tokenizer":  "standard",
          "filter": [
            "french_elision",
            "lowercase",
            "french_stop",
            "french_keywords",
            "french_stemmer"
          ]
        }
      }
    }
  }
}
```

Lâ€™utilisation du tokenizer standard est le premier problÃ¨me que jâ€™aimerais rÃ©gler. En effet, ce tokenizer est trÃ¨s simple et ne sait pas spÃ©cialement traiter des mÃ©langes dâ€™Ã©critures : il va par exemple sÃ©parer Â« Î²eta Â» en deux token (Î² et eta). Il ne sait pas non plus couper les langues non occidentalesâ€¦

Il faut lui prÃ©fÃ©rer le icu_tokenizer : plus efficace et tirant partie de la librairie ICU, qui a une connaissance Ã©tendue dâ€™Unicode. Ce tokenizer est disponible via lâ€™installation du plugin officiel analysis-icu.

Le premier filtre est french_elision, il enlÃ¨ve les articles pouvant prÃ©cÃ©der un mot, et donc dâ€™origine devient origine.

Le filtre lowercase, comme son nom lâ€™indique, permet de mettre en minuscule lâ€™intÃ©gralitÃ© du token, il est prÃ©sent par dÃ©faut dans Elasticsearch.

Arrive ensuite le filtre french_stop, qui retire les tokens tels que en, au, du, par, estâ€¦ car ils sont considÃ©rÃ©s comme du bruit â€“ prÃ©sent dans lâ€™immense majoritÃ© des documents, il Ã©tait considÃ©rÃ© peu pertinent de les conserverâ€¦ Et câ€™est bien dommage car ils peuvent apporter du sens Ã  une phrase, ou aider Ã  dÃ©partager deux documents ayant obtenus des scores Ã©gaux. Aujourdâ€™hui, avec la similaritÃ© par Okapi BM25 par dÃ©faut dans Elasticsearch 5 et la clause DSL common, il nâ€™est plus nÃ©cessaire dâ€™utiliser ce filtreâ€¯!

Pour finir, french_stemmer applique une racinisation (stemming) de nos tokens, câ€™est ce qui permet de supprimer les formes plurielles, les diffÃ©rentes conjugaisons, accord de genre sur un mot. Il existe trois algorithmes pour le franÃ§ais, mais nous conserverons le light_french utilisÃ© par dÃ©faut.

Cette derniÃ¨re Ã©tape va grandement amÃ©liorer notre collecte de document, car nous allons pouvoir trouver le mot Â« composÃ© Â» en recherchant Â« composer Â» par exemple. Mais elle fait aussi perdre du sens et de la pertinence, câ€™est pourquoi nous allons crÃ©er deux versions de notre analyzer.

Par dessus cette bonne base de travail, nous allons ajouter un meilleur support dâ€™Unicode via le filtre icu_folding. Ce filtre va faire plusieurs traitements trÃ¨s utiles :

normaliser nos textes pour sâ€™assurer que toutes les variantes dâ€™une lettre soient simplifiÃ©esâ€¯;
remplacer les lettres accentuÃ©es par leurs formes sans accentsâ€¯;
supprimer certains caractÃ¨res tels que le point mÃ©dianâ€¯;
remplacer les ligatures telles que Å“ par leurs Ã©quivalentsâ€¦

Lâ€™ajout de synonymes est aussi Ã  considÃ©rer : il serait tout Ã  fait intÃ©ressant que Â« salade Â» puisse Ãªtre trouvÃ© en recherchant Â« laitue Â», câ€™est le rÃ´le du filtre synonym. La difficultÃ© ici rÃ©side dans la constitution dâ€™un dictionnaire de correspondance pertinent.

Ce dictionnaire pourra servir plusieurs objectifs :

enrichir le vocabulaire de vos documents : salade, laitue, bataviaâ€¯;
donner de la signification aux acronymes : NASA (National Aeronautics and Space Administration), JS (JavaScript), UN (United Nations)â€¦ Ce dernier pose dâ€™ailleurs souvent problÃ¨me avec le mot Â« un Â», dâ€™oÃ¹ lâ€™importance de la casseâ€¯!
Notre domaine ici sera la cuisine rapide, les sandwichs, et nous pouvons donc utiliser un filtre synonym dans notre analyzer.

### Voici lâ€™analyzer complet avec nos modifications :

```
PUT french
{
  "settings": {
    "analysis": {
      "filter": {
        "french_elision": {
          "type": "elision",
          "articles_case": true,
          "articles": ["l", "m", "t", "qu", "n", "s", "j", "d", "c", "jusqu", "quoiqu", "lorsqu", "puisqu"]
        },
        "french_synonym": {
          "type": "synonym",
          "ignore_case": true,
          "expand": true,
          "synonyms": [
            "salade, laitue",
            "mayo, mayonnaise",
            "grille, toaste"
          ]
        },
        "french_stemmer": {
          "type": "stemmer",
          "language": "light_french"
        }
      },
      "analyzer": {
        "french_heavy": {
          "tokenizer": "icu_tokenizer",
          "filter": [
            "french_elision",
            "icu_folding",
            "french_synonym",
            "french_stemmer"
          ]
        },
        "french_light": {
          "tokenizer": "icu_tokenizer",
          "filter": [
            "french_elision",
            "icu_folding"
          ]
        }
      }
    }
  }
}
```
Nous avons deux versions :

- `french_heavy` qui va faire une analyse poussÃ©e, qui va fortement altÃ©rer les tokens mais qui va Ãªtre trÃ¨s utile pour la collecte (nous aurons beaucoup de rÃ©sultats) :

`hamburg, compos, pain, boeuf, salad, laitu`

- `french_light` qui altÃ¨re le moins possible le contenu et va nous permettre dâ€™augmenter la pertinence de nos rÃ©sultats :

`hamburger, compose, pains, boeuf, salade`

Les tokens qui ressortent lors de lâ€™indexation de notre Hamburger sont bien plus propres, et permettent donc des recherches moins prÃ©cises mais toujours pertinentes.

Sans une bonne recherche, lâ€™analyse nâ€™est rien
Avoir les bons tokens ne suffit pas : vous devrez adapter votre mapping et vos recherches.

Nous allons mettre en place un mapping simple avec un Multi Field :
```
PUT /french/_mapping/sandwich
{
  "sandwich": {
    "properties": {
      "description": {
        "type": "text",
        "analyzer": "french_light",
        "fields": {
          "stemmed": {
            "type": "text",
            "analyzer": "french_heavy"
          }
        }
      }
    }
  }
}
```
Et ajouter un second sandwich Ã  notre index :
```
{
  "name": "Burrito",
  "description": "Un burrito est une prÃ©paration culinaire remontant Ã  la fin 
du xixe siÃ¨cle originaire du Mexique. D'invention rÃ©cente, le burrito n'est 
pas un plat de la cuisine traditionnelle mexicaine. Il se compose d'une tortilla 
de farine de blÃ© garnie de divers ingrÃ©dients tels que de la viande de bÅ“uf, 
des haricots, des tomates, des Ã©pices, du piment, de l'oignon, de la salade, etc. 
On ne frit pas la tortilla, elle ne sert que d'enveloppe Ã  son contenu. 
S'il Ã©tait frit, le burrito deviendrait une chimichanga."
}
```

Recherche simple : Â« tomate Â»
Avec cette recherche, seul le Hamburger remonte, pas de Burrito, alors que nous y mettons aussi des tomates :

```
GET /french/sandwich/_search
{
  "query": {
    "match": {
      "description": "tomate"
    }
  }
}
```

Le problÃ¨meâ€¯? Nous ne recherchons que sur la version Â« light Â»â€¯! Et dans le Burrito les tomates sont au pluriel. La solution est dâ€™utiliser multi_match :

```
GET /french/sandwich/_search
{
  "query": {
    "multi_match": {
      "query": "tomate",
      "fields": ["description", "description.stemmed"]
    }
  }
}
```
Cette fois les deux documents sont prÃ©sents, et encore mieux, Hamburger a un score plus Ã©levÃ©â€¯! En effet il a le mot exact (au singulier), il est donc plus pertinent car le score des deux champs est combinÃ©.

Recherche avec un stop word : Â« sandwich du canada Â»
Cette recherche pose problÃ¨me : elle remonte le Burritoâ€¯! En effet le token du est prÃ©sent dans ce document.
```
GET /french/sandwich/_search
{
  "query": {
    "multi_match": {
      "query": "sandwich du canada",
      "fields": ["description", "description.stemmed"]
    }
  }
}
```
Pour rÃ©duire lâ€™effet de ce token du, qui va Ãªtre trÃ¨s prÃ©sent dans notre index, nous allons utiliser la clause common. Elle sÃ©pare les tokens les plus prÃ©sents dans lâ€™index des autres, et ne les utilise que pour amÃ©liorer la pertinence. Cela veut dire que si lâ€™immense majoritÃ© de mes documents possÃ¨dent le mot Â« du Â», il nâ€™aura plus dâ€™impact lors de la collecte â€“ car non pertinent.
```
GET /french/sandwich/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "common": {
            "description.stemmed": {
              "query": "sandwich du canada"
            }
          }
        }
      ],
      "should": [
        {
          "match": {
            "description": "sandwich du canada"
          }
        }
      ]
    }
  }
}
```

### TolÃ©rance aux coquilles : Â« vuande Â»

Si vous devez supporter des fautes de saisie importante, la clause Common exposÃ©e plus haut ne sera pas dâ€™une grande aideâ€¯; elle ne supporte pas la fuzziness.

Par contre avec la clause MultiMatch et lâ€™option fuzziness :
```
GET /french/sandwich/_search
{
  "query": {
    "multi_match": {
      "query": "vuande",
      "fuzziness": "AUTO",
      "fields": ["description", "description.stemmed"]
    }
  }
}
```

Nous trouvons ici nos deux sandwichsâ€¯!

### Conclusion

La recherche est un compromis entre la collecte et la pertinence : parfois il est prÃ©fÃ©rable dâ€™avoir peu de rÃ©sultats mais quâ€™ils soient trÃ¨s prÃ©cis, et dâ€™autres fois dâ€™en avoir un maximum.

</details>

